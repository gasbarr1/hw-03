{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"text-align: right;\"> &#9989; Dominic Gasbarre</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSE 202 Homework 03\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Assignment instructions\n",
    "\n",
    "Work through the following assignment, making sure to follow all of the directions and answer all of the questions.\n",
    "\n",
    "There are **25 points** possible on this assignment. Point values for each part are included in the section headers.\n",
    "\n",
    "This assignment is due at 11:59 pm on **Friday October 23rd**. It should be uploaded into the \"Homework Assignments\" submission folder for Homework 3 in your D2L webpage. Submission instructions can be found at the end of the notebook.\n",
    "\n",
    "**Hint**: It is possible you are asked to do something you are not familiar with. That's why you have internet access. Do some smart searches and see what you can find! \n",
    "\n",
    "\n",
    "### Our imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting up a repository for tracking changes (3 points)\n",
    "\n",
    "For this assignment, you're going to add it to the CMSE202 repository you created in class so that you can track your progress on the assignment and preserve the final version that you turn in. In order to do this you need to:\n",
    "\n",
    "* Navigate to your `/CMSE202/repos` repository and create a new directory called `hw-03`.\n",
    "* Move this notebook into that new directory in your repository, then add it and commit it to your repository.\n",
    " * Finally, to test that everything is working, \"git push\" the file so that it ends up in your GitHub repository.\n",
    "\n",
    "Important: Make sure you've added your TA as a collaborators\\ to your respository with \"Read\" access so that we can see your assignment.\n",
    "\n",
    "* Section 001:  tuethan\n",
    "* Section 002:  Luis-Polanco\n",
    "* Section 003:  DavidRimel\n",
    "\n",
    "Also important: Make sure that the version of this notebook that you are working on is the same one that you just added to your repository! If you are working on a different copy of the noteobok, none of your changes will be tracked.\n",
    "\n",
    "\n",
    "If everything went as intended, the file should now show up on your GitHub account CMSE202 repository under the `hw-03` directory that you just created. Periodically, you'll be asked to commit your changes to the repository and push them to the remote GitHub location. Of course, you can always commit your changes more often than that, if you wish. It can be good to get into a habit of committing your changes any time you make a significant modification, or when you stop working on the project for a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load, prepare and plot the data (5 points)\n",
    "\n",
    "In this homework we will be working with the yeast dataset and building logistic regression and k-nearest neighbors classifier class. The data file is *yeast.data* and its description is in *yeast.names*. Read the description and get a sense of the meaning of the dataset. In this part, we will load and clean up the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1 (1 point)** Load the *yeast.data* as a pandas dataframe and give appropriate names to the columns. Then drop the columns **sequence name**, **pox** and **vac**. What's the size of this dataset now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here ###\n",
    "colnames = [\"Sequence Name\",\"mcg\",\"gvh\",\"alm\",\"mit\",\"erl\",\"pox\",\"vac\",\"nuc\"]\n",
    "widths = [12,6,6,6,6,6,6,6,6,4]\n",
    "yeast_main = pd.read_fwf(\"yeast.data\",widths = widths, names = colnames)\n",
    "yeast_main = yeast_main.drop([\"Sequence Name\",\"pox\",\"vac\"],axis=1)\n",
    "print(yeast_main.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2 (1 point)** Find the number of unqiue entries in the class label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here ###\n",
    "yeast_main['nuc'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3 (1 point)** We are only interested in data with label **CYT (cytosolic or cytoskeletal)** and **MIT (mitochondrial)**. Make a new dataframe containing\n",
    "data with only these two types of labels, and redefine label **CYT** into **0**, and **MIT** into **1**. What's the size of the dataset now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here ###\n",
    "yeast = yeast_main[yeast_main['nuc'].isin([\"CYT\",\"MIT\"])].replace({\"CYT\":0,\"MIT\":1})\n",
    "print(f'the size of the dataset is {yeast.shape[0]} by {yeast.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4 (2 points)** Make a scatter plot including every sample in the dataset with: the mcg feature on the x-axis, the gvh feature on the y-axis, and different colors for each class label. Make your observation. Are the two classes distinguishable using only those two features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here ###\n",
    "plt.figure(figsize = (12,7))\n",
    "plt.scatter(yeast['mcg'],yeast['gvh'],c = yeast['nuc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In the next part we will build a logistic regression model for the data classification.\n",
    "\n",
    "## Part 3: Prepare data and build the logistic regression model (7 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1 (2 points)** Apply the \"train_test_split\" function in the *sklearn* package to split the data in 70% for training and 30% for testing.  Using common variable names like x_train, y_train, x_test and y_test might help later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here ###\n",
    "yeast_train, yeast_test, label_train, label_test = train_test_split(yeast[[\"mcg\",\"gvh\",\"alm\",\"mit\",\"erl\"]],yeast[\"nuc\"], test_size = .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2 (2 points)** Perform the logistic regression. \n",
    "* Discuss your results. How well does your model fit your data? What evidence are you using to make the determination? \n",
    "* Based on the P values under \"P > |z|\", which two features **in this dataset** are the least significant and can be dropped?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here ###\n",
    "\n",
    "#clf = sklearn.linear_model.LogisticRegression(random_state= 0).fit(yeast_train,label_train)\n",
    "\n",
    "#param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "#                  'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "#clf = GridSearchCV(SVC(kernel='linear', class_weight='balanced'), param_grid, n_jobs=-1)\n",
    "#clf = clf.fit(yeast_train,label_train)\n",
    "model  = sm.Logit(label_train,yeast_train)\n",
    "result = model.fit(maxiter=200)\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly this was a terrible model. I might have done something wrong because of how poor this model was. The pseudo R-squared is only .287 which is absolutely garbage when it comes to predicting. mcg, gvh, and alm seemed to be fine predictors though so maybe dropping the others will be more fruitful.\n",
    "\n",
    "Also I noticed that when performing the logit algorithm multiple times I would get very different answers; sometimes I would get to maxiter = 200, but sometimes it would terminate successfully after only 6 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3 (3 points)** Drop the two least important features found in the previous question and perform the logistic regression again. Then use the use the `sklearn.metrics` we imported at the top and run the `accuracy_score` on the 0/1 predicted label and the test labels, and print the accuracy of this model.\n",
    "\n",
    "* Discuss your results. How well does your reduced model fit your data? What evidence are you using to make the determination?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here ###\n",
    "yeast_train, yeast_test, label_train, label_test = train_test_split(yeast[[\"mcg\",\"gvh\",\"alm\"]],yeast[\"nuc\"], test_size = .3)\n",
    "model  = sm.Logit(label_train,sm.add_constant(yeast_train))\n",
    "result = model.fit(maxiter=100)\n",
    "print(result.summary())\n",
    "predictions = np.array(result.predict(sm.add_constant(yeast_test)))\n",
    "predictions = np.rint(predictions)\n",
    "metrics.accuracy_score(predictions,label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one worked a lot better - it actually optimized properly! Unfortunately though, there was only an 81.7% chance of correctly predicting the class, which is far from perfect. I also noticed that the r-squared wasn't really much better, which is odd but if it works, it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "In the next part we will be building a class that will use the k-nearest neighbors algorithm (kNN) to make predictions on the same dataset. From the previous part (logistic regression), you have selected **4 features** that are important for classification. We will **only** use those 4 features in this part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: KNN classifier, cross-validation and hyperparameter tuning (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.1 (3 points)** Test drive the KNN classifier. Use the same train and test data you created in question 3.4 to build a KNN classifier with K=3. \n",
    "- make a `KNeighborsClassifier` with an argument of `n_neighbors=3`. This returns a knn classifier (let's just call it `knn`)\n",
    "- call `knn.fit` on the training data\n",
    "- use `knn.predict` on the testing data to generate the predicted values.\n",
    "- print the confusion matrix.\n",
    "- print the train and test score using `knn.score`.\n",
    "- plot the ROC curve with the diagonal (the \"chance line\") also labeled. Using `sklearn.metrics`, print the `auc` for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here ###\n",
    "import sklearn\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(yeast_train,label_train)\n",
    "predictions = knn.predict(yeast_test)\n",
    "cm = sklearn.metrics.confusion_matrix(label_test,predictions)\n",
    "score = knn.score(yeast_test,label_test)\n",
    "y_scores = knn.predict_proba(yeast_test)\n",
    "fpr, tpr, threshold = metrics.roc_curve(label_test, predictions)\n",
    "auc = round(sklearn.metrics.auc(fpr,tpr),3)\n",
    "plt.plot(fpr, tpr, color='darkorange', label=f'ROC curve (area = {auc})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Fold Cross-Validation\n",
    "Cross-validation is when the dataset is randomly split up into ‘k’ groups. One of the groups is used as the test set and the rest are used as the training set. The model is trained on the training set and scored on the test set. Then the process is repeated until each unique group as been used as the test set.\n",
    "For example, for 5-fold cross validation, the dataset would be split into 5 groups, and the model would be trained and tested 5 separate times so each group would get a chance to be the test set. This can be seen in the graph below.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*NyvaFiG_jXcGgOaouumYJQ.jpeg\" width=700px>\n",
    "\n",
    "The train-test-split method we used in earlier is called ‘holdout’. Cross-validation is better than using the holdout method because the holdout method score is dependent on how the data is split into train and test sets. Cross-validation gives the model an opportunity to test on multiple splits so we can get a better idea on how the model will perform on unseen data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.2 (2 points)** Look up `cross_val_score` in `sklearn.model_selection`. We will still use n_neighbors=3, and  a cross-validation value of 5. `cross_val_score` takes in our k-NN model and our data as parameters. Then it splits our data into 5 groups and fits and scores our data 5 seperate times, recording the accuracy score in an array each time. We will save the accuracy scores in the cv_scores variable. Then find the average of the cv_scores, that will provide you a more accurate understanding of the accuracy of the model.\n",
    "\n",
    "* Discuss your results. How well do your models fit your data? \n",
    "* What are you using to judge that fit (i.e., how should we think about the accuracy score as a measure of quality of the model)?\n",
    "* How does the quality of the KNN model compare to logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here ###\n",
    "cv_scores = cross_val_score(knn,yeast[[\"mcg\",\"gvh\",\"alm\"]],yeast[\"nuc\"])\n",
    "print(sum(cv_scores)/len(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would say that both models fit the data pretty well - they're both right around 80% accuracy. I wouldn't really say one is better than the other, even though the logistic model has slightly better accuracy, because of the randomness of these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "\n",
    "Almost all machine learning models have hyperparamters. Hyperparameters are setting(s) in the model that the user needs to choose before learning takes place. For example, in k-nearest neighbors, the number of neighbors to consider  n_neighbors, is the hyperparameter. An important task in machine learning is hyperparameter tuning, which is finding the optimal hyperparmeter. We will now explore the optimal choice of this parameter for this dataset.\n",
    "\n",
    "**Question 4.3 (3 points)** Consider the range of `n_neighbors` from 1 to 100, and fix the cross-validation value to be 5. \n",
    "- For each value of n_neighbors, compute the means of the cv_scores. \n",
    "- Make a plot with the x-axis being n_neighbors, y-axis being the mean of cv_scores.\n",
    "- Find the optimal choice of n_neighbors with the largest value of the mean of cv_scores.\n",
    "\n",
    "Discuss your results\n",
    "* How does the quality of this model compare to the earlier models that you made with KNN and logisitic regression?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here ###\n",
    "x = []\n",
    "y = []\n",
    "for i in range(1,101):\n",
    "    x.append(i)\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(yeast_train,label_train)\n",
    "    cv_scores = cross_val_score(knn,yeast[[\"mcg\",\"gvh\",\"alm\"]],yeast[\"nuc\"])\n",
    "    y.append(sum(cv_scores)/len(cv_scores))\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(x,y,c='r')\n",
    "plt.xlabel(\"Neighbors\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use a more efficient method: `GridSearchCV` in `sklearn.model_selection` to find the optimal n_neighbors.\n",
    "\n",
    "**Question 4.4 (2 points)** Look up `GridSearchCV` in `sklearn.model_selection`. We will still use a cross-validation value of 5.  Use `best_params_` in `GridSearchCV` to find the optimal n_neighbors. Does it agree with the results from question 4.3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here ###\n",
    "param_grid = {\"n_neighbors\": range(1,101) }\n",
    "clf = GridSearchCV(KNeighborsClassifier(),param_grid, n_jobs=-1).fit(yeast_train,label_train)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a slightly lower value than in 4.3; 15 instead of the 20 I visually estimated. Still close, and both are of similar accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Assignment wrap-up\n",
    "\n",
    "Please fill out the form that appears when you run the code below.  **You must completely fill this out in order to receive credit for the assignment!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\n",
    "\"\"\"\n",
    "<iframe \n",
    "\tsrc=\"https://docs.google.com/forms/d/e/1FAIpQLSc0IBD2mdn4TcRyi-KNXVtS3aEg6U4mOFq2MOciLQyEP4bg1w/viewform?usp=sf_link\" \n",
    "\twidth=\"800px\" \n",
    "\theight=\"600px\" \n",
    "\tframeborder=\"0\" \n",
    "\tmarginheight=\"0\" \n",
    "\tmarginwidth=\"0\">\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations, you're done!\n",
    "\n",
    "Submit this assignment by uploading it to the course Desire2Learn web page.  Go to the \"Homework Assignments\" folder, find the dropbox link for Homework 3, and upload your notebook **and the script you wrote**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
